{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4670d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import minari\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ae85b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Dataset Flattening (unchanged)\n",
    "# ------------------------------------------------------------\n",
    "def flatten_minari_dataset(dataset):\n",
    "    obs_list, act_list, rew_list, next_obs_list, term_list = [], [], [], [], []\n",
    "    for ep in dataset.iterate_episodes():\n",
    "        obs_data = ep.observations\n",
    "        acts = np.asarray(ep.actions)\n",
    "        rews = np.asarray(ep.rewards)\n",
    "        terms = np.asarray(ep.terminations)\n",
    "        if len(acts) < 1:\n",
    "            continue\n",
    "\n",
    "        if isinstance(obs_data, dict):\n",
    "            obs = np.concatenate([np.asarray(v) for v in obs_data.values()], axis=-1)\n",
    "        elif isinstance(obs_data[0], dict):\n",
    "            obs = [np.concatenate([v.flatten() for v in o.values()]) for o in obs_data]\n",
    "            obs = np.stack(obs)\n",
    "        else:\n",
    "            obs = np.asarray(obs_data)\n",
    "        if obs.shape[0] <= 1:\n",
    "            continue\n",
    "\n",
    "        s = obs[:-1]\n",
    "        s2 = obs[1:]\n",
    "        a = acts[:len(s)]\n",
    "        r = rews[:len(s)]\n",
    "        done = terms[:len(s)] if len(terms) == len(s) else np.zeros_like(r)\n",
    "        obs_list.append(s)\n",
    "        act_list.append(a)\n",
    "        rew_list.append(r.reshape(-1, 1))\n",
    "        next_obs_list.append(s2)\n",
    "        term_list.append(done.reshape(-1, 1))\n",
    "\n",
    "    s_all  = torch.tensor(np.concatenate(obs_list, axis=0), dtype=torch.float32)\n",
    "    a_all  = torch.tensor(np.concatenate(act_list, axis=0), dtype=torch.float32)\n",
    "    r_all  = torch.tensor(np.concatenate(rew_list, axis=0), dtype=torch.float32)\n",
    "    s2_all = torch.tensor(np.concatenate(next_obs_list, axis=0), dtype=torch.float32)\n",
    "    d_all  = torch.tensor(np.concatenate(term_list, axis=0), dtype=torch.float32)\n",
    "    print(f\"Flattened dataset: {s_all.shape[0]} transitions\")\n",
    "    print(f\"State dim: {s_all.shape[1]}, Action dim: {a_all.shape[1]}\")\n",
    "    return s_all, a_all, r_all, s2_all, d_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec7c759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Networks\n",
    "# ------------------------------------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = MLP(state_dim, action_dim)\n",
    "    def forward(self, s): return torch.tanh(self.net(s))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = MLP(state_dim + action_dim, 1)\n",
    "    def forward(self, s, a): return self.net(torch.cat([s, a], -1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae28df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Double Critic Loss with Weighting W(r+w_c)\n",
    "# ------------------------------------------------------------\n",
    "def critic_loss(Q1, Q2, Q1_targ, Q2_targ, pi, batch,\n",
    "                gamma=0.99, lam_f=0.01, g_a=1.0,\n",
    "                lam_Q=1e-3, wc=0.0):\n",
    "    s, a, r, s2, _ = [x.to(device) for x in batch]\n",
    "\n",
    "    # Target using min of target critics (TD3 style)\n",
    "    with torch.no_grad():\n",
    "        next_a = pi(s2)\n",
    "        target_Q = r + gamma * torch.min(Q1_targ(s2, next_a), Q2_targ(s2, next_a))\n",
    "\n",
    "    # Critic values\n",
    "    Q1_val, Q2_val = Q1(s, a), Q2(s, a)\n",
    "    pi_a = pi(s)\n",
    "\n",
    "    # λ_Q regularization\n",
    "    reg_term = lam_Q * (pi_a - a).pow(2).sum(-1, keepdim=True)\n",
    "\n",
    "    # Standard Bellman errors\n",
    "    bellman1 = (target_Q - Q1_val - reg_term).pow(2)\n",
    "    bellman2 = (target_Q - Q2_val - reg_term).pow(2)\n",
    "\n",
    "    # -------- First-Order Terms --------\n",
    "    s.requires_grad_(True); a.requires_grad_(True)\n",
    "    Q1_sa = Q1(s, a)\n",
    "    grad_s = autograd.grad(Q1_sa.sum(), s, create_graph=True)[0]\n",
    "    grad_a = autograd.grad(Q1_sa.sum(), a, create_graph=True)[0]\n",
    "    s_dot = s2 - s\n",
    "\n",
    "    # Nearby-state consistency\n",
    "    e = torch.rand_like(s[..., :1])\n",
    "    near_s = s - e * (s2 - s)\n",
    "    near_s.requires_grad_(True)\n",
    "    Q_near = Q1(near_s, a)\n",
    "    grad_s_near = autograd.grad(Q_near.sum(), near_s, create_graph=True)[0]\n",
    "\n",
    "    state_cons = - (grad_s * s_dot).sum(-1) / (grad_s.norm(dim=-1)*s_dot.norm(dim=-1)+1e-8)\n",
    "    near_state_cons = - (grad_s_near * s_dot).sum(-1) / (grad_s_near.norm(dim=-1)*s_dot.norm(dim=-1)+1e-8)\n",
    "    total_state_cons = state_cons + near_state_cons\n",
    "\n",
    "    mask = (grad_a.norm(dim=-1)**2 > g_a).float()\n",
    "    action_cons = mask * (grad_a.norm(dim=-1)**2)\n",
    "\n",
    "    first_order = total_state_cons + action_cons\n",
    "\n",
    "    # Reward weighting W(r + w_c)\n",
    "    W = 100.0 * (r + wc)   # as in paper for AntMaze\n",
    "    loss = (bellman1 + bellman2 + lam_f * W * first_order.unsqueeze(-1)).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75ea90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Actor Loss (same)\n",
    "# ------------------------------------------------------------\n",
    "def actor_loss(Q1, pi, batch, lam_pi=1e-3):\n",
    "    s, a, _, _, _ = [x.to(device) for x in batch]\n",
    "    pi_a = pi(s)\n",
    "    reg = (pi_a - a).pow(2).sum(-1, keepdim=True)\n",
    "    return (-Q1(s, pi_a) + lam_pi * reg).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b10de79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Training Loop\n",
    "# ------------------------------------------------------------\n",
    "def train_offline_rl(s, a, r, s2, d, epochs=3000, batch_size=512,\n",
    "                     lam_f=0.01, g_a=1.0, lam_Q=1e-3, wc=0.0):\n",
    "    state_dim, action_dim = s.shape[1], a.shape[1]\n",
    "    actor = Actor(state_dim, action_dim).to(device)\n",
    "    Q1, Q2 = Critic(state_dim, action_dim).to(device), Critic(state_dim, action_dim).to(device)\n",
    "    Q1_targ, Q2_targ = Critic(state_dim, action_dim).to(device), Critic(state_dim, action_dim).to(device)\n",
    "    Q1_targ.load_state_dict(Q1.state_dict())\n",
    "    Q2_targ.load_state_dict(Q2.state_dict())\n",
    "\n",
    "    optA = optim.Adam(actor.parameters(), lr=3e-4)\n",
    "    optC = optim.Adam(list(Q1.parameters()) + list(Q2.parameters()), lr=3e-4)\n",
    "\n",
    "    gamma, tau = 0.99, 0.005\n",
    "    N = s.shape[0]\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        idx = torch.randint(0, N, (batch_size,))\n",
    "        batch = (s[idx], a[idx], r[idx], s2[idx], d[idx])\n",
    "\n",
    "        # Critic update\n",
    "        optC.zero_grad()\n",
    "        Lc = critic_loss(Q1, Q2, Q1_targ, Q2_targ, actor, batch,\n",
    "                         gamma, lam_f, g_a, lam_Q, wc)\n",
    "        Lc.backward(); optC.step()\n",
    "\n",
    "        # Actor update\n",
    "        optA.zero_grad()\n",
    "        La = actor_loss(Q1, actor, batch)\n",
    "        La.backward(); optA.step()\n",
    "\n",
    "        # Soft target updates\n",
    "        with torch.no_grad():\n",
    "            for q, qt in zip([Q1, Q2], [Q1_targ, Q2_targ]):\n",
    "                for p, pt in zip(q.parameters(), qt.parameters()):\n",
    "                    pt.data.mul_(1 - tau).add_(tau * p.data)\n",
    "\n",
    "        if ep % 100 == 0:\n",
    "            print(f\"Epoch {ep:4d} | Critic {Lc.item():.4f} | Actor {La.item():.4f}\")\n",
    "\n",
    "    return actor, Q1, Q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c31f786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened dataset: 1000000 transitions\n",
      "State dim: 8, Action dim: 2\n",
      "Epoch    0 | Critic 0.0878 | Actor -0.0042\n",
      "Epoch  100 | Critic 0.0102 | Actor 0.0427\n",
      "Epoch  200 | Critic 0.0152 | Actor 0.0256\n",
      "Epoch  300 | Critic 0.0105 | Actor 0.0381\n",
      "Epoch  400 | Critic 0.0116 | Actor 0.0180\n",
      "Epoch  500 | Critic 0.0046 | Actor -0.0142\n",
      "Epoch  600 | Critic 0.0062 | Actor -0.0151\n",
      "Epoch  700 | Critic 0.0093 | Actor -0.0281\n",
      "Epoch  800 | Critic 0.0108 | Actor -0.0519\n",
      "Epoch  900 | Critic 0.0150 | Actor -0.0428\n",
      "Epoch 1000 | Critic 0.0081 | Actor -0.0656\n",
      "Epoch 1100 | Critic 0.0152 | Actor -0.0786\n",
      "Epoch 1200 | Critic 0.0103 | Actor -0.0718\n",
      "Epoch 1300 | Critic 0.0099 | Actor -0.0941\n",
      "Epoch 1400 | Critic 0.0129 | Actor -0.1023\n",
      "Epoch 1500 | Critic 0.0156 | Actor -0.1451\n",
      "Epoch 1600 | Critic 0.0124 | Actor -0.1435\n",
      "Epoch 1700 | Critic 0.0138 | Actor -0.1537\n",
      "Epoch 1800 | Critic 0.0139 | Actor -0.1540\n",
      "Epoch 1900 | Critic 0.0168 | Actor -0.1616\n",
      "Epoch 2000 | Critic 0.0186 | Actor -0.1887\n",
      "Epoch 2100 | Critic 0.0155 | Actor -0.2094\n",
      "Epoch 2200 | Critic 0.0162 | Actor -0.2446\n",
      "Epoch 2300 | Critic 0.0239 | Actor -0.2581\n",
      "Epoch 2400 | Critic 0.0179 | Actor -0.2704\n",
      "Epoch 2500 | Critic 0.0190 | Actor -0.2865\n",
      "Epoch 2600 | Critic 0.0258 | Actor -0.3108\n",
      "Epoch 2700 | Critic 0.0167 | Actor -0.3259\n",
      "Epoch 2800 | Critic 0.0241 | Actor -0.3601\n",
      "Epoch 2900 | Critic 0.0237 | Actor -0.3822\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Full Run\n",
    "# ------------------------------------------------------------\n",
    "dataset = minari.load_dataset(\"D4RL/pointmaze/umaze-v2\", download=True)\n",
    "s, a, r, s2, d = flatten_minari_dataset(dataset)\n",
    "\n",
    "actor, Q1, Q2 = train_offline_rl(s, a, r, s2, d,\n",
    "                                 epochs=3000, batch_size=1024,\n",
    "                                 lam_f=0.01, g_a=1.0, lam_Q=1e-3, wc=0.0)\n",
    "\n",
    "torch.save(actor.state_dict(), \"actor_fullpaper.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a45690b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Reward: 5.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ Evaluation\n",
    "# ------------------------------------------------------------\n",
    "eval_env = dataset.recover_environment(eval_env=True)\n",
    "\n",
    "def flatten_obs(obs):\n",
    "    if isinstance(obs, dict):\n",
    "        return np.concatenate([obs[\"observation\"], obs[\"achieved_goal\"], obs[\"desired_goal\"]])\n",
    "    return obs\n",
    "\n",
    "def evaluate_policy(env, actor, episodes=5):\n",
    "    total = 0\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        ep_rew = 0\n",
    "        for t in range(300):\n",
    "            obs_vec = flatten_obs(obs)\n",
    "            obs_t = torch.tensor(obs_vec, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            act = actor(obs_t).cpu().detach().numpy()[0]\n",
    "            obs, rew, done, trunc, _ = env.step(act)\n",
    "            ep_rew += rew\n",
    "            if done or trunc:\n",
    "                break\n",
    "        total += ep_rew\n",
    "    return total / episodes\n",
    "\n",
    "avg_reward = evaluate_policy(eval_env, actor)\n",
    "print(f\"Average Evaluation Reward: {avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "430dd547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 50 episodes — success_rate: 0.340, mean_return: 21.480, std_return: 50.212, mean_length: 300.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_metrics(env, actor, episodes=100, max_steps=300):\n",
    "    successes = 0\n",
    "    returns = []\n",
    "    lengths = []\n",
    "    trajectories = []   # to optionally visualize later\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        ep_ret = 0.0\n",
    "        traj = []\n",
    "        for t in range(max_steps):\n",
    "            # flatten dict obs\n",
    "            if isinstance(obs, dict):\n",
    "                obs_vec = np.concatenate([obs[\"observation\"], obs[\"achieved_goal\"], obs[\"desired_goal\"]])\n",
    "            else:\n",
    "                obs_vec = obs\n",
    "            obs_t = torch.tensor(obs_vec, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                act = actor(obs_t).cpu().numpy()[0]\n",
    "            obs, rew, done, trunc, info = env.step(act)\n",
    "            ep_ret += rew\n",
    "            # store position for visualization (achieved_goal)\n",
    "            if isinstance(obs, dict):\n",
    "                traj.append(obs[\"achieved_goal\"].copy())\n",
    "            else:\n",
    "                traj.append(obs.copy())\n",
    "            if done or trunc:\n",
    "                break\n",
    "        returns.append(ep_ret)\n",
    "        lengths.append(len(traj))\n",
    "        trajectories.append(np.array(traj))\n",
    "        # success detection: env-specific; assume reward>0 indicates success\n",
    "        if ep_ret > 0:\n",
    "            successes += 1\n",
    "\n",
    "    success_rate = successes / episodes\n",
    "    print(f\"Evaluated {episodes} episodes — success_rate: {success_rate:.3f}, mean_return: {np.mean(returns):.3f}, std_return: {np.std(returns):.3f}, mean_length: {np.mean(lengths):.1f}\")\n",
    "    return success_rate, returns, lengths, trajectories\n",
    "\n",
    "# Example usage:\n",
    "success_rate, returns, lengths, trajectories = evaluate_metrics(eval_env, actor, episodes=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f168fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
